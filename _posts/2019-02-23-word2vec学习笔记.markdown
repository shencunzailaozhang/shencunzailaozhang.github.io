---
layout: post
title: word2vec学习笔记
date: 2019-02-23
---
（说在前面，下面这篇笔记里有很多都是网上的内容，但考虑到没人看，所以我就不注明出处了）
由于最近在做病历处理方面的东西，需要提取病人口述的病症，根据病症预测病人患某种病的概率是多大。为了能让计算机处理病症，需要先将它们向量化，向量化有两种方式
# one-hot向量
虽然onehot向量简单，但是向量维度和词语数量相等，而且因为存在大量的0，会导致浪费计算资源。另一个弊端是它无法反映词语之间的联系，所以转而用分布式表示。
# 分布式表示
分布式表示维度可控，不会因为词语过多而导致维度灾难，而且可以很好地反映词语之间的关系，通过计算欧式距离可以算出词语之间的相近程度。
分布式表示有以下几种
- 基于矩阵的分布式表示
- 基于聚类的分布式表示
- 基于神经网络的分布式表示（词嵌入word embedding）  
主要将基于神经网络的分布式表示，基于神经网络的分布式表示又有以下几种模型
  - Neural Network Language Model ，NNLM
  - Log-Bilinear Language Model， LBL
  - Recurrent Neural Network based Language Model，RNNLM
  - Collobert 和 Weston 在2008 年提出的 C&W 模型
  - Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型  
这些和word2vec有什么关系呢？
### word2vec是包含CBOW和Skip-gram的训练词向量的工具。  
所以学word2vec其实是在学CBOW和Skip-gram。CBOW是给定上下文单词来预测中心词，Skip-gram是给定中心词来预测上下文单词，如图所示![](/assets/images/structure.jpg)
## 问题及答案
**1. CBOW和Skip-gram的输入和输出分别是什么形式？如何更新得到想要的词向量？**  
  答：先说一下CBOW的网络结构，如图所示![](/assets/images/CBOWstructure.jpg)  
  输入层到隐藏层之间没有激活函数，隐藏层到输出层之间激活函数为softmax。  
  CBOW的输入是上下文单词的one-hot向量,这些向量分别乘以一个权重矩阵然后再相加求平均，输出是一个经过神经网络计算后的概率向量；  
  **将这个概率向量与真实中心词经过神经网络计算后的概率向量作比较，算出损失值，目标是让损失值达到最小**。  
  这样的话，模型中参数就会得到更新（**这个参数就是词向量，看Notes**）,然后再取其它的上下文词，重复上述步骤。  
  ![](/assets/images/CBOW1.JPG)  
  skip-gram网络结构，如图所示  
  ![](/assets/images/skipgramstructure.jpg)  
  skipgram的输入是中心词的one-hot向量，输出是多个上下文单词的概率向量（**它们的概率向量都是一样的**），然后分别与它们所对应的真实词的one-hot向量作比较，最小化损失值。  
  这样的话，模型中参数就会得到更新（**这个参数就是词向量，看Notes**）,然后再取其它的中心词，重复上述步骤，如图所示  
  ![](/assets/images/skipgram.jpg)  
## Notes:
- 无论是训练CBOW还是Skip-gram模型，我们想要的其实是模型中的参数（更确切的说是输入矩阵的参数），这个参数就是词向量，所以常常会看到博客中说“词向量只是训练模型后的副产物”。
- 关于这两个模型，还有两个优化的trick，分别是hierarchical softmax和negative sampling，它们是可以加快计算速度的，所以也得学。

