---
layout: post
title: 代码解读
date: 2019-04-10
---    
# 前言
好久没写博客了，但我都有写在纸上，我觉得写博客有点耗时间，所以只有空闲下来才会写写，今天想写点最近的进展。  
今年五一放了四天假，我没有出去玩，都一直在实现我的idea，本来调了三天都没结果，本打算放弃，结果昨晚程序突然可以用了，有点欣喜，这是不是让我不要轻易放弃的信号呢？  
# 网络图  
我在原始的BERT-BiLSTM-CRF基础上加入了聚焦法（但后来我觉得这不叫cnn，把它改名为局部注意力，但是和传统的注意力也不一样，所以暂且叫它聚焦法），还想加入局部自注意力模型，还没去实现。    
![总体网络图](/assets/images/new_idea0.JPG)  
![聚焦法](/assets/images/new_idea1.JPG)  
![局部自注意力法](/assets/images/new_idea2.JPG)
# 解读  
聚焦法说白了就是用矩阵相乘来将一部分的向量信息编码到某一个向量中，先设置滑动窗口（2N+1）来去固定数量的向量，然后一直滑动，一直编码，最终会得到一个和原矩阵相同大小的矩阵，这个矩阵中每一列都编码了左右N个向量的信息，这样做的目的是重点捕捉当前字和周围字的关系，想通过这么做来发掘词语之间的关系，这样做的好处是比较优雅，比起网上其它的同时捕捉字和词关系的方法都简单的多。  
![聚焦法代码实现流程图](/assets/images/new_idea3.JPG)
