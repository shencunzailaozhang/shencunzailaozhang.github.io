---
layout: post
title: 40天学习历程
date: 2019-03-09
--- 
开学已经一个月了，虽然感觉看了很多东西，但很少有深入理解，所以先记录下所学的东西，后期再深入学习。
# 词向量 
因为需要处理病历，将病人的症状提取出来，然后向量化，放入模型中进行预测病人患病的概率有多大。
有最原始的one-hot，bag-of-words，但是当特征很多的时候，这样会导致向量太稀疏，而且也无法反映出词语之间的关系，于是学比较先进的word2vec，word2vec的学习花了大概一周，还跑了程序，word2vec中包含了两个模型：CBOW和Skipgram，word2vec将这两种方法都封装了，所以只需要调用就可以了，很方便，它可以将词语映射为固定长度的向量，对于大语料来说，向量的维度要大一些，一般为300~500维。
生成词向量之后就将它处理了一下，每个病人的病症为一行，生成csv文件。
# python界面
本想着如果做一个系统的话，我的任务是输入病症，系统会输出病人得脑梗的概率，于是我还学习了python做界面，有点麻烦，所有东西都得自己写代码，没有直接拖动图标建界面的软件，做了一两天，最后也算是做出来了。
# naive bayes 
单词向量化之后便想着用什么模型来实现，首先想到的是ANN，因为无论是分类或是回归，神经网络都可以很好地完成，用以前的模型试了一下（用到了csv文件），代码跑得通，但由于样本少所以预测结果太差。然后学习了小样本情况下该用哪种模型，发现了朴素贝叶斯模型。学习朴素贝叶斯，首先得具备概率方面的知识，像条件概率，条件独立假设公式等，学了几天，试了一下我的数据（用朴素贝叶斯模型不需要之前生成的词向量），效果还可以。 
# RNN及其变体 
学完了词向量之后，一度以为我已经完成了老师交给我的任务，后来我深入想了一下我的任务，发现太简单了，根本没什么工作量，于是想到了学RNN，这是自然语言处理必备知识，学RNN那就一定会引申出它的变体LSTM等，也需要学习，RNN的确可以做很多事情，例如自动生成对联或唐诗等有趣的东西，但这些程序都需要服务器来跑，苦于没有服务器，便没有实践。 
# 注意力机制 
学习LSTM时一定会看到一个东西***“注意力机制”***，这是为了更合理地分配encoder中最后的输出向量而设定的，现在也有很多种变体，不过我觉得还是有创新余地的。注意力机制大概学了一周，还看了论文，提出了自己的想法。 
*** 
我一直是边学习边结合我的任务，直到我发现我的任务好像没法做，
  - 预测脑梗，缺少反例
  - 预测分数，病历里面的症状不足以预测分数，而且分数直接由医生根据指标给出，比较机械化，没有什么价值
  - 预测诊疗计划，这个是我觉得唯一可以做的，可以做一个类似自动问答的系统，病人输入自己的情况，然后会得到相应的答案。。。
问题一大堆，到此便进入了停滞期，因为我没有机器跑程序，也没有了看代码的帮手，一度不知道我该干什么，于是去问了老师我现在该干嘛，老师给我指明了方向。 
*** 
# 命名实体识别 
命名实体识别又称NER（Named Entity Recognition），它是信息提取、问答系统、句法分析、机器翻译、面向Semantic Web的元数据标注等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。这样看来，它的应用较为广泛，研究并改进它具有很重大的实际意义，所以这周主要在学NER，NER也牵扯到很多其它的知识，主要是概率方面的，像隐马尔科夫模型（HMM）和条件随机场（CRF）等，现在NER最常用的方法是BiLstm+CRF，还下载并运行了一个代码，基于Keras框架的，看了几天，发现最关键的部分没怎么看懂，因为过于错综复杂，所以还得更加深入模型，完全了解模型的流程才能够更轻松地继续后续工作。


