---
layout: post
title: 代码解读
date: 2019-05-04
---    
# 前言
好久没写博客了，但我都有写在纸上，我觉得写博客有点耗时间，所以只有空闲下来才会写写，今天想写点最近的进展。  
今年五一放了四天假，我没有出去玩，都一直在实现我的idea，本来调了三天都没结果，本打算放弃，结果昨晚程序突然可以用了，有点欣喜，这是不是让我不要轻易放弃的信号呢？  
# 网络图  
我在原始的BERT-BiLSTM-CRF基础上加入了聚焦法（但后来我觉得这不叫cnn，把它改名为局部注意力，但是和传统的注意力也不一样，所以暂且叫它聚焦法），还想加入局部自注意力模型，还没去实现。    
![总体网络图](/assets/images/new_idea0.JPG)  
![聚焦法](/assets/images/new_idea1.JPG)  
![局部自注意力法](/assets/images/new_idea2.JPG)
# 解读  
聚焦法说白了就是用矩阵相乘来将一部分的向量信息编码到某一个向量中，先设置滑动窗口（2N+1）来去固定数量的向量，然后一直滑动，一直编码，最终会得到一个和原矩阵相同大小的矩阵，这个矩阵中每一列都编码了左右N个向量的信息，这样做的目的是重点捕捉当前字和周围字的关系，想通过这么做来发掘词语之间的关系，这样做的好处是比较优雅，比起网上其它的同时捕捉字和词关系的方法都简单的多。  
![聚焦法代码实现流程图](/assets/images/new_idea3.JPG)  
实际操作起来并没有那么简单，因为还要考虑到双向lstm的存在，我的聚焦法输出的矩阵要与BiLSTM输出的矩阵相加，然后送入CRF中，所以得完全搞清楚BiLSTM处理输入矩阵的流程，在此基础上才能更好地加入聚焦法。  
# 后续工作    
- 学习自注意力机制，并加入到我的模型中  
- 每周一篇论文，从论文中找创新点  
