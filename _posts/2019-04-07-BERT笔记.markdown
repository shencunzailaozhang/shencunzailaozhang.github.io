---
layout: post
title: BiLSTM+CRF笔记
date: 2019-04-07
---  
# 前言  
虽然早就知道[Transformer](https://github.com/tensorflow/tensor2tensor)、[BERT](https://github.com/google-research/bert#fine-tuning-with-bert)、[paddlepaddle](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE)，也知道它们很好用，但觉得很复杂就一直拖着没去尝试，在看完了ACL2018和NER相关的[论文](https://arxiv.org/abs/1805.02023)后（[项目地址](https://github.com/shencunzailaozhang/LatticeLSTM)），我终于决定尝试新模型了，网上现在做NER的模型大多是BiLSTM+CRF，区别就在于对字/词向量的提取、处理等。ACL这篇论文是将字向量与词向量融合起来当作网络的输入，但和BERT比起来，它对向量的处理还是略显幼稚，虽然BERT是基于字符的，并没有融入词语信息，但我觉得BERT中的自注意力机制让它可以捕获上下文意思，但还没经过实验，我也不知道哪个比较好用，但BERT已经在众多数据集上达到了sota水准，所以我要开始深入BERT了。
# 先验知识  
## 博客
[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)BERT发展史    
[The Illustrated Transformer【译】](https://blog.csdn.net/yujianmin1990/article/details/85221271)详细解释了Transformer
## 论文  
[Attention Is All You Need](https://arxiv.org/abs/1706.03762)解释了Transformer中的注意力机制    
# FAQ(frequently asked questions)  
**Q:到底是直接用BERT做NER还是BERT只用来生成字向量，再将字向量置于其它模型中？**    

**A:** 我倾向于后者，BERT是用来捕捉字/句子之间深度关系的，然后输出是向量，这大概就是BERT的作用了，之后再将字/句子向量输入到其它模型中。但是想得到字向量，得先把输入改造成BERT需要的样子。    

**Q:对于NER而言，如何预处理数据呢？**     

**A:** unknown for now    

**Q:BERT中各个文件的作用**    

**A:** 
- [modeling.py解读](https://www.jianshu.com/p/d7ce41b58801)  
- [tokenization.py](https://www.jianshu.com/p/22e462f01d8c)tokenization是对原始句子内容的解析，分为BasicTokenizer和WordpieceTokenizer两个。  
  - BasicTokenizer的主要是进行unicode转换、标点符号分割、小写转换、中文字符分割、去除重音符号等操作，最后返回的是关于词的数组（中文是字的数组）。  
  - WordpieceTokenizer的目的是将合成词分解成类似词根一样的词片。例如将"unwanted"分解成["un", "##want", "##ed"]这么做的目的是防止因为词的过于生僻没有被收录进词典最后只能以[UNK]代替的局面，因为英语当中这样的合成词非常多，词典不可能全部收录。  
  - FullTokenizer的作用就很显而易见了，对一个文本段进行以上两种解析，最后返回词（字）的数组，同时还提供token到id的索引以及id到token的索引。这里的token可以理解为文本段处理过后的最小单元。    
- [create_pretraining_data.py](https://www.jianshu.com/p/22e462f01d8c)对原始数据进行转换，原始数据本是无标签的数据，通过句子的拼接可以产生句子关系的标签，通过MASK可以产生标注的标签，其本质是语言模型的应用    
- [run_pretraining.py](https://www.jianshu.com/p/22e462f01d8c)：在执行预训练的时候针对以上两种标签分别利用bert模型的不同输出部件，计算loss，然后进行梯度下降优化。



# 参考博客  
<https://www.jianshu.com/p/22e462f01d8c>
