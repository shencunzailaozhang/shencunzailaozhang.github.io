---
layout: post
title: BERT笔记
date: 2019-04-07
---  
# 前言  
虽然早就知道[Transformer](https://github.com/tensorflow/tensor2tensor)、[BERT](https://github.com/google-research/bert#fine-tuning-with-bert)、[paddlepaddle](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE)，也知道它们很好用，但觉得很复杂就一直拖着没去尝试，在看完了ACL2018和NER相关的[论文](https://arxiv.org/abs/1805.02023)后（[项目地址](https://github.com/shencunzailaozhang/LatticeLSTM)），我终于决定尝试新模型了，网上现在做NER的模型大多是BiLSTM+CRF，区别就在于对字/词向量的提取、处理等。ACL这篇论文是将字向量与词向量融合起来当作网络的输入，但和BERT比起来，它对向量的处理还是略显幼稚，虽然BERT是基于字符的，并没有融入词语信息，但我觉得BERT中的自注意力机制让它可以捕获上下文意思，但还没经过实验，我也不知道哪个比较好用，但BERT已经在众多数据集上达到了sota水准，所以我要开始深入BERT了。
# 先验知识  
## 博客
[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)BERT发展史    
[The Illustrated Transformer【译】](https://blog.csdn.net/yujianmin1990/article/details/85221271)详细解释了Transformer
## 论文  
[Attention Is All You Need](https://arxiv.org/abs/1706.03762)解释了Transformer中的注意力机制    
# FAQ(frequently asked questions)  
**Q:到底是直接用BERT做NER还是BERT只用来生成字向量，再将字向量置于其它模型中？**    

**A:** 我倾向于后者，BERT是用来捕捉字/句子之间深度关系的，然后输出是向量，这大概就是BERT的作用了，之后再将字/句子向量输入到其它模型中。但是想得到字向量，得先把输入改造成BERT需要的样子。    

**Q:对于NER而言，如何预处理数据呢？**     

**A:** unknown for now    

**Q:BERT源码解读**    

**A:** 
- [modeling.py](https://www.jianshu.com/p/d7ce41b58801)  
- [tokenization.py](https://www.jianshu.com/p/22e462f01d8c)  
tokenization是对原始句子内容的解析，分为BasicTokenizer和WordpieceTokenizer两个。 
  - BasicTokenizer的主要是进行unicode转换、标点符号分割、小写转换、中文字符分割、去除重音符号等操作，最后返回的是关于词的数组（中文是字的数组）。  
  - WordpieceTokenizer的目的是将合成词分解成类似词根一样的词片。例如将"unwanted"分解成["un", "##want", "##ed"]这么做的目的是防止因为词的过于生僻没有被收录进词典最后只能以[UNK]代替的局面，因为英语当中这样的合成词非常多，词典不可能全部收录。  
  - FullTokenizer的作用就很显而易见了，对一个文本段进行以上两种解析，最后返回词（字）的数组，同时还提供token到id的索引以及id到token的索引。这里的token可以理解为文本段处理过后的最小单元。    
- [create_pretraining_data.py](https://www.jianshu.com/p/22e462f01d8c)  
对原始数据进行转换，原始数据本是无标签的数据，通过句子的拼接可以产生句子关系的标签，通过MASK可以产生标注的标签，其本质是语言模型的应用    
- [run_pretraining.py](https://www.jianshu.com/p/22e462f01d8c)  
在执行预训练的时候针对以上两种标签分别利用bert模型的不同输出部件，计算loss，然后进行梯度下降优化。  
- [run_squad.py](https://www.jianshu.com/p/116bfdb9119a)（run_squad.py和run_classifier.py是bert中fine-tune的例子）  
- [run_classifier.py](https://blog.csdn.net/u014108004/article/details/84142035#commentBox)（讲的很好，还涉及了如何把代码改造成自己需要的样子）
# 代码  
- [bert-as-service](https://github.com/hanxiao/bert-as-service)  可以生成字/词/句子向量，但维度相同，不知道这有何意义，但字向量至少是可以用的，而且极其简单。  
- 另一个基于Keras的代码，也可以用，用起来也很简单。 作者还写了一个实例 
  - [项目地址](https://github.com/BrikerMan/Kashgari)  
  - [实例](https://eliyar.biz/nlp_chinese_bert_ner/)  
- [使用预训练语言模型BERT做中文NER(bert-chinese-ner-master)](https://github.com/ProHiryu/bert-chinese-ner),这个代码没有加入BiLSTM和CRF，但是它是最基础的，之后的都在这上面改。    
- [BERT-BiLSTM-CRF-NER](https://github.com/FuYanzhe2/Name-Entity-Recognition),这个代码加入了BiLSTM和CRF，路径：F:\google下载\BERT相关\Name-Entity-Recognition-master\BERT-BiLSTM-CRF-NER。    
- [BERT-BiLSTM-CRF-NER-master](https://github.com/macanv/BERT-BiLSTM-CRF-NER),这个代码也加入了BiLSTM和CRF，但是和上面的相比功能好像更全，作者还生成了软件包，里面集成了bert-as-service。  
## 注意事项  
  - 代码不能直接用，除了按照作者README.md中写的那样操作外，还需要在项目中加入output文件夹，output文件夹中还要加入result_dir文件夹。  
# 我的理解  
参考BERT中自带的fine-tune的实例（run_squad.py和run_classifier.py）可以发现，想把BERT用于NER，首先得把自己的输入改造成BERT所需的格式，然后写一个Processor类，这个类就和run_classifier.py中的Processor结构相同，但是里面的方法要重写。除此之外，还可以加上自己的损失函数（在create_model中），还可以加入CRF层等。



# 参考博客  
<https://www.jianshu.com/p/22e462f01d8c>  
<https://blog.csdn.net/u014108004/article/details/84142035#commentBox>
